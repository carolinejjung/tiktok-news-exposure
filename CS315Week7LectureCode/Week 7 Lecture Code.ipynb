{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c975a0c6-bdfb-4756-b87e-ca68a05f5336",
   "metadata": {},
   "source": [
    "# Week 7 Lecture Code: Word Embeddings / Clustering / tSNE visualization\n",
    "\n",
    "**March 2024**\n",
    "\n",
    "This is code that was discussed in the class notes for the two lectures in Week 7.\n",
    "\n",
    "**Table of Content**\n",
    "\n",
    "1. [Installing packages](#sec1)\n",
    "2. [Generating Word Embeddings and heatmaps for cosine similarity](#sec2)\n",
    "3. [K-means clustering with word embeddings](#sec3)\n",
    "4. [Using the Elbow method to find the best k for k-means](#sec4)\n",
    "5. [t-SNE visualization](#sec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15700c2-2db5-4123-adbf-87755d31dd4c",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>\n",
    "## 1. Installing Packages\n",
    "\n",
    "This notebook contains several packages that we are using for the first time. Additionally, we might need to use these packages for the project too, thus, it's good if we install them within the virtual environment of the project.  \n",
    "\n",
    "However, if we install the packages within Jupyter, they are installed in our system-wide Python installation. We need to take some extra steps to ensure that the new packages are installed for our project-specific virtual environment. Here is what you need to do:\n",
    "\n",
    "1. Make sure that you have created the virtual environment for Project 2 (about collecting data with PykTok). You can find the instructions under Week 7 Programming Activities.\n",
    "2. Open a terminal window, navigate to the folder where you created your virtual environment, and then type `source .project2/bin/activate`.\n",
    "3. Install the following package: `pip install ipykernel`, to manage virtual environments within Jupyter notebooks.\n",
    "4. Register our virtual environment with Jupyter: `python -m ipykernel install --user --name=.project2 --display-name=\"CS315 Project2`\n",
    "5. At this point close this notebook and then restart it again.\n",
    "6. When you reopen the notebook, find the button \"Python 3 (ipykernel)\" on the top right corner. Click on it, you should see the following dialog, where you should select our virtual environment that we gave the display name CS315 Project2: ![Screenshot: Kernel selection](https://cs.wellesley.edu/~eni/cs315/choose-kernel.png)\n",
    "7. Before moving on, run the cell below, just to make sure that you are really running the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b598aa2-6f7b-4bda-a206-92738cb72868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb68a5c-ab89-4854-b330-07b3566fcf4a",
   "metadata": {},
   "source": [
    "If all the prior steps were successful, you should be able to see something similar to the path below, including `.project2/bin/python`\n",
    "![Screenshot: Path](https://cs.wellesley.edu/~eni/cs315/path-confirmation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6bfb92-281f-489e-93c4-10412413f213",
   "metadata": {},
   "source": [
    "Now you are ready to continue with the following installations for libraries used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5f3054-6cc6-4f72-94d6-6b83014b1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow tensorflow_hub scikit-learn seaborn plotly nbformat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57827f06-5769-4441-8d93-73118488f8a8",
   "metadata": {},
   "source": [
    "If the installation was successful, you are ready to continue. If you run into issues, let me know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd28d25-de89-4f58-9c6b-dd47061d95e9",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>\n",
    "## 2. Generating word embeddings\n",
    "\n",
    "We will use the `tensorflow` library that loads the Universal Sentence Encoder from Google. Running the cells below takes several seconds, this is why it's good to have a notebook that you keep open and running to avoid having to repeat these steps everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c1b7aa-73e2-4b3c-b9a8-b04f75c89489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd02bf8-f384-40e8-83dc-a0ed68975ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0317bba7-bb46-4214-b3f2-e2574131ea88",
   "metadata": {},
   "source": [
    "Let's see what `embed` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77205152-c887-43b7-a717-10dd72f27d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655b878-13fc-47be-ad5f-d585b7649d81",
   "metadata": {},
   "source": [
    "It seems like instance of a class that is loading a saved model for the user.\n",
    "\n",
    "We know that `embed` expects a list of words, thus, even if we want the embedding of one single word, we put it in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33063607-2fe3-4193-84f1-0011ab8ce2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the word embedding of a single word\n",
    "embed([\"apple\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1761e9eb-7a8b-448e-ad6e-c4c3bbcb88e4",
   "metadata": {},
   "source": [
    "From the result, we can see that we got a tensor of shape 1x512, meaning a single row with 512 columns (the dimensions for the word representation). A tensor is a mathematical term for a multi-dimensional array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d916a-434f-4387-9aa0-105c731a3876",
   "metadata": {},
   "source": [
    "Let's get the embeddings for a few pairs of words that we know are similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e24667-0b6c-4df3-b84a-5303947c7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['king', 'queen', 'radio', 'TV', 'bike', 'car', 'Boston', 'London', 'lake', 'river']\n",
    "\n",
    "embeddings = embed(words)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049cf657-ebcb-4eb4-b9c6-23bcd27f0b5b",
   "metadata": {},
   "source": [
    "As we can see, we got one embedding for each word as a vector of 512 dimensions. We will calculate the cosine similarity between these words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc9362-9d1b-4e8e-9b5b-38b072166286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def cosineSimilarity(vec1, vec2):\n",
    "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "    V1 = np.array(vec1)\n",
    "    V2 = np.array(vec2)\n",
    "    cosine = np.dot(V1, V2)/(norm(V1)*norm(V2))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4cd67d-1457-4394-a8ec-8e40fe2bd00a",
   "metadata": {},
   "source": [
    "Now that we have this function, we will call it to calculate the similarity between all pairs of words, as part of a new function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8337354-1f4d-4234-bf1c-574d4d3a6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwiseSimilarity(embeddings):\n",
    "    \"\"\"Given a matrix of embeddings for words or sentences,\n",
    "    calculate the cosine similarity for each pair.\n",
    "    \"\"\"\n",
    "    simMatrix = []\n",
    "    for vec1 in embeddings:\n",
    "        simRow = []\n",
    "        for vec2 in embeddings:\n",
    "            simRow.append(cosineSimilarity(vec1, vec2))\n",
    "        simMatrix.append(simRow)\n",
    "    return simMatrix\n",
    "    \n",
    "simMatrix = pairwiseSimilarity(embeddings)\n",
    "print(simMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b529cb-4be6-48de-be1f-aa8deca6e0a8",
   "metadata": {},
   "source": [
    "Let's generate a heatmap to see the similarity between the pairs of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413dcbc-5994-4edd-a35a-180536a2cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def drawHeatmap(labels, simMtrx, plotTitle):\n",
    "    \"\"\"Draws a heatmap for the similarity matrix.\n",
    "    \"\"\"\n",
    "    sns.set(font_scale=0.9)\n",
    "    g = sns.heatmap(\n",
    "          simMtrx, # similarity matrix with the cosine sim values\n",
    "          xticklabels=labels,\n",
    "          yticklabels=labels,\n",
    "          vmin=0,\n",
    "          vmax=1,\n",
    "          cmap=\"YlOrRd\")\n",
    "    g.set_xticklabels(labels, rotation=90)\n",
    "    g.set_title(plotTitle, fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8bf97a-ef82-468e-8c42-2d984a24b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drawHeatmap(words, simMatrix, \"Similarity for Word Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6d950-ba9f-47a8-8225-87e1d49ecbe1",
   "metadata": {},
   "source": [
    "**Optional Challenge: Find most similar words**\n",
    "\n",
    "While the heat map gives us some information, is hard to see the most simlar words. We can write a Python function to find the most similar pairs. I wrote one that I can call like this:\n",
    "\n",
    "`findUniqueTopPairs(simMatrix, words)[:10]`\n",
    "\n",
    "And the results are the following:\n",
    "```\n",
    "[(0.58856946, 'king', 'queen'),\n",
    " (0.56984055, 'bike', 'car'),\n",
    " (0.49752563, 'Boston', 'London'),\n",
    " (0.471712, 'lake', 'river'),\n",
    " (0.45328128, 'TV', 'radio'),\n",
    " (0.45171037, 'car', 'radio'),\n",
    " (0.43531647, 'TV', 'car'),\n",
    " (0.3785724, 'bike', 'river'),\n",
    " (0.3519483, 'London', 'king'),\n",
    " (0.35168734, 'car', 'river')]\n",
    "```\n",
    "\n",
    "Notice how our original 5 pairs are at the top, and pairs like 'car' and 'radio' are still related. One can also understand the relationshp between 'London' and 'king' (based on the history of the many kings who lived in London, although for the past 70 years there was a queen in the palace). Anyway, we can see how the similarity is going down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086377d4-d2f8-42ee-b272-31e563385714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e9813-8631-499a-908a-7dcc42d9eea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2aab7697-d611-4795-88eb-eeadd4f0294f",
   "metadata": {},
   "source": [
    "<a id=\"sec3\"></a>\n",
    "## 3. K-means clustering with word embeddings\n",
    "\n",
    "In lecture we showed the clustering for the TikTok hashtags collected from posts, but for connecting this section to the t-SNE visualization section below, we will show here the clustering for our own list of news hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67740a-4061-4100-a7dd-872b7a23fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the news hashtags\n",
    "import json\n",
    "news = json.load(open('news-hashtags.json'))\n",
    "news[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e8d39-318a-4c8d-aaf8-efaaa479c28d",
   "metadata": {},
   "source": [
    "**Step 1: Get the word embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b27a2d6-5999-4fe3-9efe-3eb3bdccf1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember that we loaded the model in the prior section\n",
    "newsEmbed = embed(news)\n",
    "newsEmbed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da04f34-32e6-4cb6-9861-f5deec14d99b",
   "metadata": {},
   "source": [
    "**Step 2: Perform clustering with a fixed k value**\n",
    "\n",
    "For this time, we will perform clustering with k=15. The code will take a few seconds (or more) to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d57f6-be4d-49f1-a224-e15de04cc4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 15 # number of clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "clusters = kmeans.fit_predict(newsEmbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a737b1ba-2193-489b-9c49-d5b2e99f020a",
   "metadata": {},
   "source": [
    "Let's see what the results look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f413849-ddef-46f8-8394-a7577a86d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73337644-2a34-4901-81bf-dd2e27d6aca1",
   "metadata": {},
   "source": [
    "They are indices of the clusters. For each elemenent in our `news` list, the `clusters` array indicates in which of the 15 clusters the item has been assigned. We can then use this information to find out which words are in which cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a0fad-6fd1-4744-bd28-63f6020f2737",
   "metadata": {},
   "source": [
    "We will print out the composition of each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d3343-9151-46e5-95d3-c1e081279cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    cluster_words = [news[j] for j in range(len(news)) if clusters[j] == i]\n",
    "    print(cluster_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f283a3-bb5c-4cc9-8fdf-6cce66fc2fc4",
   "metadata": {},
   "source": [
    "As we discussed when we looked at the t-SNE visualization in class, some groups of words are really meaningful, for example, Cluster 5, Cluster 6, Cluster 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d0461-69f6-42e1-bc3d-012392953006",
   "metadata": {},
   "source": [
    "<a id=\"sec4\"></a>\n",
    "## 4. Using the Elbow method to find the best k\n",
    "\n",
    "We discussed that having to pick k is a limitation of K-means clustering. Below, we show one method known as the Elbow method to try to pick the best k. If you want to learn about the method, here is [a short 4-minute video](https://www.youtube.com/watch?v=ht7geyMAFfA) to explain it. It is called the Elbow method because k is selected at the lowest point that marks something that looks like an elbow. This is the point where the sum of squared distances from the cluster centroid stops decreasing rapidly and enters a phase of a constant-paced decline, toward 0. This sum of squared distances of all points from the cluster centroid is known as inertia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73271b-7338-4940-90eb-8cbd93323dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbowMethod(embeddings, maxK):\n",
    "    \"\"\"\n",
    "    Implements the Elbow method for finding most optimal k.\n",
    "    It keeps track of a measure named \"inertia\" for each cluster.\n",
    "    \"\"\"\n",
    "    sumSquaredDistances = []\n",
    "    kValues = list(range(1, maxK))\n",
    "    for k in kValues:\n",
    "        km = KMeans(n_clusters=k, random_state=42)\n",
    "        km = km.fit(embeddings)\n",
    "        sumSquaredDistances.append(km.inertia_)\n",
    "    \n",
    "    # plot the line to identify the elbow\n",
    "    plt.plot(kValues, sumSquaredDistances, 'ro-')\n",
    "    plt.xlabel('k')\n",
    "    plt.xticks(kValues)\n",
    "    plt.ylabel('Sum of squared distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75975e-3c67-4bff-8d51-bc6e49c01ac3",
   "metadata": {},
   "source": [
    "Let's try below 20 values for k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e8fcc-d4db-41e2-bd3f-4da0e0d04b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbowMethod(newsEmbed, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3ffeb-336c-40eb-9004-a9746595830f",
   "metadata": {},
   "source": [
    "In this plot, inertia stops rapidly decreasing at the k=3 and enters a phase of steady decline. So we will pick k=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b738d-e873-4f5e-8bf7-5b22df792c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "clusters = kmeans.fit_predict(newsEmbed)\n",
    "\n",
    "clusters[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4416b3-880e-4c60-8c54-5264078866ea",
   "metadata": {},
   "source": [
    "Notices the indices of the clusters: 0, 1, 2. We can check quickly how many items are in each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc1ac9-c2a0-4bea-9ef9-52856510e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84f73d-0805-4706-aedc-21e1099b9b8d",
   "metadata": {},
   "source": [
    "So, it looks like the clusters have roughly the same size: 0 and 2 have about 40 items and cluster 1 has 50 items. Let's visualize the clusters using t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6402d-25e9-4959-9db9-3c025fd7dc15",
   "metadata": {},
   "source": [
    "<a id=\"sec5\"></a>\n",
    "## 5. t-SNE visualization\n",
    "\n",
    "To create the t-SNE visualization, we need a few steps to prepare the data first. We work directly with the embeddings and then use the cluster membership as a column that will allow us to color the dots in the scatterplot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a5c21-8759-4fc9-8072-b2634209abfa",
   "metadata": {},
   "source": [
    "**Step 1: Run the TSNE algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab1c9d-abcb-4a25-9eef-7d07f28fbcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)  \n",
    "tsne_results = tsne.fit_transform(newsEmbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf7526c-d9b6-4b26-9cd7-bde75d5b4247",
   "metadata": {},
   "source": [
    "**Step 2: Create a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f028c41c-bd4f-474b-9e6b-d2201e3e72e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(tsne_results, columns=['tsne_1', 'tsne_2'])\n",
    "df['hashtag'] = news  \n",
    "df['cluster'] = clusters # the cluster indices where each news hashtags belong\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39a0cb-f63f-4093-80b7-fe84ca52945e",
   "metadata": {},
   "source": [
    "**Step 3: Generate Plotly visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca5f2b-543a-4227-a8ee-b1969ef63f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.scatter(df, x='tsne_1', y='tsne_2', text='hashtag', color=\"cluster\", color_continuous_scale=\"BlueRed\")\n",
    "\n",
    "# Format what to show next to the markers\n",
    "fig.update_traces(textposition='top center', \n",
    "                  mode='markers+text', \n",
    "                  textfont=dict(size=6))\n",
    "\n",
    "fig.update_layout(title='Embeddings of TikTok News Hashtags', width=800, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925f8c2-96bc-48bf-91b3-c200ea63250b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS315 Project2",
   "language": "python",
   "name": ".project2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
